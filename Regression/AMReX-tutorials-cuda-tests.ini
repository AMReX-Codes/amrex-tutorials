[main]
testTopDir = /scratch/regtester/RegTesting/rt-AMReX-tutorials
webTopDir  = /scratch/regtester/RegTesting/rt-AMReX-tutorials/web

MAKE = make
sourceTree = C_Src
# Don't set it too high. Currently Ubuntu has a bug that causes parallel make fail
numMakeJobs = 8

COMP = g++
FCOMP = gfortran
add_to_c_make_command = TEST=TRUE USE_ASSERTION=TRUE
add_to_f_make_command = TEST=t

purge_output = 1

# suiteName is the name prepended to all output directories
suiteName = AMReX-tutorials-GPU

reportActiveTestsOnly = 1

# Add "GO UP" link at the top of the web page?
goUpLink = 1

# email
sendEmailWhenFail = 1
#emailTo = weiqunzhang@lbl.gov
emailTo = weiqunzhang@lbl.gov, ASAlmgren@lbl.gov, atmyers@lbl.gov
emailBody = Check https://ccse.lbl.gov/pub/GpuRegressionTesting/AMReX-tutorials/ for more details.

# MPIcommand should use the placeholders:
#   @host@ to indicate where to put the hostname to run on
#   @nprocs@ to indicate where to put the number of processors
#   @command@ to indicate where to put the command to run
#
# only tests with useMPI = 1 will run in parallel
# nprocs is problem dependent and specified in the individual problem
# sections.

#MPIcommand = mpiexec -host @host@ -n @nprocs@ @command@
MPIcommand = mpiexec -n @nprocs@ @command@
MPIhost =

[AMReX]
dir = /home/regtester/git/amrex
# PLEASE DO NOT CHANGE THIS.  IF YOU CHANGE THIS PLEASE REMEMBER TO
# CHANGE IT BACK TO DEVELOPMENT.
branch = "development"

[source]
dir = /home/regtester/git/amrex-tutorials
branch = "main"

# individual problems follow

[CNS-Sod]
addToCompileString = USE_CUDA=TRUE
buildDir = GPU/CNS/Exec/Sod
inputFile = inputs-rt
dim = 3
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
testSrcTree = C_Src

[CNS-RT]
addToCompileString = USE_CUDA=TRUE
buildDir = GPU/CNS/Exec/RT
inputFile = inputs-rt
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
testSrcTree = C_Src

[MLMG_PoisLev]
addToCompileString = USE_CUDA=TRUE
buildDir = LinearSolvers/ABecLaplacian_C
inputFile = inputs-rt-poisson-lev
dim = 3
restartTest = 0
useMPI = 1
numprocs = 2
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[MLMG_ABecCom]
addToCompileString = USE_CUDA=TRUE
tolerance = 1.e-13
runtime_params = gpu_regtest=1
buildDir = LinearSolvers/ABecLaplacian_C
inputFile = inputs-rt-abeclap-com
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[MLMG_NodalPoisson]
addToCompileString = USE_CUDA=TRUE
tolerance = 1.e-12
runtime_params = gpu_regtest=1
buildDir = LinearSolvers/NodalPoisson
inputFile = inputs-rt
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[Tracers]
addToCompileString = USE_CUDA=TRUE
buildDir = Amr/Advection_AmrLevel/Exec/SingleVortex
inputFile = inputs.tracers
runtime_params = particles.do_tiling=0
dim = 2
restartTest = 0
useMPI = 1
numprocs = 2
useOMP = 0
probinFile = probin
compileTest = 0
compareParticles = 1
doVis = 0
testSrcTree = C_Src
